{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256a28a-6623-4934-8904-31ba8df52663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4cd29-5936-4ad1-b180-bd7544aafec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "Rents = pd.read_csv('Rents & Transactions/rents.csv', delimiter=';', low_memory=False)\n",
    "Transactions = pd.read_csv('Rents & Transactions/transactions.csv', delimiter=';', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bf2c8-a556-4728-9c9d-381632dacd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rents.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c5bfe5-6e3b-4fc7-9556-d34cc332d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transactions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43696d62-f040-4c3c-8f23-babab7a086c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rents['Property Size (sq.m)'] = pd.to_numeric(Rents['Property Size (sq.m)'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416764f7-07f6-4293-9bb3-01d530b1272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rents['Parking'] = pd.to_numeric(Rents['Parking'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7721fe-ff5e-4346-9e4b-d9761ca70af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transactions['Property Size (sq.m)'] = pd.to_numeric(Transactions['Property Size (sq.m)'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b66ff-338e-4a5c-88c2-88a0bf4a5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transactions['Parking'] = pd.to_numeric(Transactions['Parking'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64108c-cbbd-4885-ad87-d012ae318aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared columns with high similarity to include in the merge\n",
    "merge_keys = [\n",
    "    'Property ID', 'Is Free Hold?', 'Nearest Metro', \n",
    "    'Nearest Mall', 'Nearest Landmark', \n",
    "    'Usage', 'Area', 'Property Type', 'Property Sub Type', 'Property Size (sq.m)', 'Parking', 'Master Project', 'Project'\n",
    "]\n",
    "\n",
    "# Perform the merge using these keys\n",
    "merged_data = pd.merge(\n",
    "    Rents, Transactions,\n",
    "    on=merge_keys,  # Merge on the identified keys\n",
    "    how='outer',     # Retain all rows from Rents\n",
    "    suffixes=('', '_Transactions')  # Add suffix for Transactions columns\n",
    ")\n",
    "\n",
    "# Check the merged data\n",
    "print(\"Merged Data Information:\")\n",
    "print(merged_data.info())\n",
    "\n",
    "print(\"\\nPreview of Merged Data:\")\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e9915-bf7c-4e9d-a61a-9a0c98004bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric columns stored as strings\n",
    "merged_data['Annual Amount'] = pd.to_numeric(merged_data['Annual Amount'], errors='coerce')\n",
    "merged_data['Contract Amount'] = pd.to_numeric(merged_data['Contract Amount'], errors='coerce')\n",
    "merged_data['Amount'] = pd.to_numeric(merged_data['Amount'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97a76f-75c1-4a36-8460-2d977e3a4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average prices the previous month/week (for the same kind of property)\n",
    "\n",
    "# Step 1: Convert dates to datetime format\n",
    "merged_data['Registration Date'] = pd.to_datetime(merged_data['Registration Date'], errors='coerce')\n",
    "\n",
    "# Step 2: Extract year, month, and week from Transaction Date\n",
    "merged_data['Year'] = merged_data['Registration Date'].dt.year\n",
    "merged_data['Month'] = merged_data['Registration Date'].dt.month\n",
    "merged_data['Week'] = merged_data['Registration Date'].dt.isocalendar().week\n",
    "\n",
    "# Step 3: Define property characteristics for grouping\n",
    "property_characteristics = ['Area', 'Property Type', 'Property Sub Type', 'Usage', 'Is Free Hold?']\n",
    "\n",
    "# Step 4: Calculate average prices for the previous month\n",
    "merged_data['Prev_Month_Avg_Price'] = (\n",
    "    merged_data.groupby(property_characteristics + ['Year', 'Month'])['Amount']\n",
    "    .transform(lambda x: x.shift().mean())\n",
    ")\n",
    "\n",
    "# Step 5: Calculate average prices for the previous week\n",
    "merged_data['Prev_Week_Avg_Price'] = (\n",
    "    merged_data.groupby(property_characteristics + ['Year', 'Week'])['Amount']\n",
    "    .transform(lambda x: x.shift().mean())\n",
    ")\n",
    "\n",
    "# Verify the new columns\n",
    "print(merged_data[['Prev_Month_Avg_Price', 'Prev_Week_Avg_Price']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f5a25-3105-40f5-92df-1a4c7cbabb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Prev_Month_Avg_Price'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd9f2f3-d473-4b98-acbd-bf966bb689f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Prev_Week_Avg_Price'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf2f04-44ae-4e0e-ab66-f6a13d882f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates to datetime format\n",
    "merged_data['Start Date'] = pd.to_datetime(merged_data['Start Date'], errors='coerce')\n",
    "\n",
    "# Convert dates to datetime format\n",
    "merged_data['End Date'] = pd.to_datetime(merged_data['End Date'], errors='coerce')\n",
    "\n",
    "# Convert dates to datetime format\n",
    "merged_data['Transaction Date'] = pd.to_datetime(merged_data['Transaction Date'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6465cb9-c6b0-447f-83a6-b46bfa77ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Registration Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f48e00-14e9-40ea-9a78-76262a16ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea675c-e4bc-4fa8-adf2-f516077c9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0e24c-6be8-4405-a8d1-fb1225a1e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['Room(s)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d5334-dc12-4a06-ab67-96f02eab383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Sample 10% of the data\n",
    "sampled_data = merged_data.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Display info and preview of sampled data\n",
    "print(sampled_data.info())\n",
    "print(sampled_data.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c83644-bf9f-4067-9db7-e0f13f436aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b30186-8616-4df4-b1d0-ff022db4382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# 1. Prepare the data\n",
    "# Drop rows where 'Annual Amount' is missing\n",
    "merged_data = merged_data.dropna(subset=['Annual Amount'])\n",
    "\n",
    "# Define features and target\n",
    "X = merged_data.drop(columns=['Annual Amount', 'Ejari Contract Number', 'Registration Date', 'Start Date', 'End Date', 'Transaction Date'])\n",
    "y = merged_data['Annual Amount']\n",
    "\n",
    "# 2. Train-Test Split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc5353-83c0-4869-b8ef-913239994136",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Identify categorical columns\n",
    "categorical_cols = [\n",
    "    'Transaction Type', 'Transaction sub type', 'Registration type', \n",
    "    'Is Free Hold?', 'Usage', 'Area', 'Property Type', \n",
    "    'Property Sub Type', 'Room(s)', 'Parking', 'Nearest Metro', \n",
    "    'Nearest Mall', 'Nearest Landmark', 'Master Project', 'Project'\n",
    "]\n",
    "\n",
    "# Ensure the categorical columns are treated as category dtype\n",
    "for col in categorical_cols:\n",
    "    if col in X_train.columns:  # Check if column exists in the data\n",
    "        X_train[col] = X_train[col].astype('category')\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "# Categorical features: Get the indices of the categorical columns in X_train\n",
    "cat_features = [i for i, col in enumerate(X_train.columns) if X_train[col].dtype.name == 'category']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75a1f7-9876-4a09-b047-a94854e57bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost Model\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Handle NaN in categorical features\n",
    "cat_columns = X_train.select_dtypes(include=['object']).columns\n",
    "for col in cat_columns:\n",
    "    X_train[col] = X_train[col].fillna('Unknown')\n",
    "    X_test[col] = X_test[col].fillna('Unknown')\n",
    "\n",
    "# Categorical features: Get the indices of the categorical columns in X_train\n",
    "cat_features = [i for i, col in enumerate(X_train.columns) if X_train[col].dtype == 'object']\n",
    "\n",
    "# Step 1: Define the CatBoost model\n",
    "model = CatBoostRegressor(iterations=200,  # Number of boosting iterations\n",
    "                          learning_rate=0.1,  # Learning rate\n",
    "                          depth=6,  # Tree depth\n",
    "                          cat_features=cat_features,  # List of categorical feature indices\n",
    "                          random_seed=42,  # Random seed for reproducibility\n",
    "                          verbose=200)  # Print progress every 200 iterations\n",
    "\n",
    "# Step 2: Train the model\n",
    "model.fit(X_train, y_train, cat_features=cat_features)\n",
    "\n",
    "# Step 3: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)  # Calculate R² Score\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"MSE: {mse:.4f}\")   # Mean Squared Error\n",
    "print(f\"RMSE: {rmse:.4f}\")  # Root Mean Squared Error\n",
    "print(f\"R² Score: {r2:.4f}\")  # R-squared score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0dda6-f155-4f40-a96f-9abbc5f4b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importance from the trained model\n",
    "feature_importance = model.get_feature_importance()\n",
    "\n",
    "# Create a DataFrame to map feature names with importance scores\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort the importance values in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance of Annual Amount for CatBoost (Rents and Transactions Data)')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important features on top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bfc585-4b6c-4883-8f52-895304b50c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Model\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Step 1: Convert categorical columns to category dtype\n",
    "categorical_columns = ['Version', 'Area', 'Is Free Hold?', 'Property Type', \n",
    "                       'Property Sub Type', 'Usage', 'Nearest Metro', \n",
    "                       'Nearest Mall', 'Nearest Landmark', 'Master Project', 'Project',\n",
    "                      'Transaction Number', 'Transaction Type', 'Transaction sub type',\n",
    "                       'Registration type', 'Room(s)'\n",
    "                      ]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "# Step 2: Define the LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(objective='regression', \n",
    "                              num_iterations=200,  # Number of boosting iterations\n",
    "                              learning_rate=0.1,  # Learning rate\n",
    "                              max_depth=6,  # Tree depth\n",
    "                              verbose=-1,\n",
    "                              random_state=42)  # Random seed for reproducibility\n",
    "\n",
    "# Step 3: Train the model with categorical features\n",
    "lgb_model.fit(X_train, y_train, categorical_feature=categorical_columns)\n",
    "\n",
    "# Step 4: Make predictions on the test set\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "mse_lgb = mean_squared_error(y_test, y_pred_lgb)\n",
    "rmse_lgb = mse_lgb ** 0.5\n",
    "r2_lgb = r2_score(y_test, y_pred_lgb)  # Calculate R² Score\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"MSE: {mse_lgb:.4f}\")   # Mean Squared Error\n",
    "print(f\"RMSE: {rmse_lgb:.4f}\")  # Root Mean Squared Error\n",
    "print(f\"R² Score: {r2_lgb:.4f}\")  # R-squared score\n",
    "\n",
    "\n",
    "# Step 6: Plot feature importance for LightGBM\n",
    "lgb_feature_importance = lgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to map feature names with importance scores\n",
    "lgb_feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': lgb_feature_importance\n",
    "})\n",
    "\n",
    "# Sort the importance values in ascending order\n",
    "lgb_feature_importance_df = lgb_feature_importance_df.sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot LightGBM feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(lgb_feature_importance_df['Feature'], lgb_feature_importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance of Annual Amount for LightGBM (Rents and Transactions)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b663d-ce17-4020-a839-b1f4b276ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Step 1: Check all categorical columns in X_train\n",
    "cat_columns = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Ensure that 'Version' and any other categorical column are included in cat_features\n",
    "cat_features = [i for i, col in enumerate(X_train.columns) if col in cat_columns]\n",
    "\n",
    "# Handle NaN in categorical features\n",
    "for col in cat_columns:\n",
    "    # Convert column to 'category' type if it's not already\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "    # Check if 'Unknown' is already a category, and add it if not\n",
    "    if 'Unknown' not in X_train[col].cat.categories:\n",
    "        X_train[col] = X_train[col].cat.add_categories('Unknown')\n",
    "    if 'Unknown' not in X_test[col].cat.categories:\n",
    "        X_test[col] = X_test[col].cat.add_categories('Unknown')\n",
    "\n",
    "    # Fill NaN values with 'Unknown'\n",
    "    X_train[col] = X_train[col].fillna('Unknown')\n",
    "    X_test[col] = X_test[col].fillna('Unknown')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f266cb-70eb-4b2e-beff-6b15ad4b23b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e273f-0e9b-49b9-8db6-b36d401e70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abdda1-37a6-4ee9-8ecf-d739fefce12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding supplementary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d9a72-518b-42a3-8ef6-4f1ab4e2a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Consumer_Price_Index_Annually = pd.read_csv('Consumer Price Index/Consumer_Price_Index_Annually.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce6b5f-6ac3-4ff1-aed5-b5743b81e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter relevant columns from CPI data\n",
    "cpi_data = Consumer_Price_Index_Annually[['MEASURE', 'TIME_PERIOD', 'OBS_VALUE']].copy()\n",
    "cpi_data.rename(columns={'TIME_PERIOD': 'Year', 'OBS_VALUE': 'CPI_Value'}, inplace=True)\n",
    "\n",
    "# Handle duplicate entries: aggregate by Year and MEASURE (e.g., take the mean of OBS_VALUE)\n",
    "cpi_data = cpi_data.groupby(['Year', 'MEASURE'], as_index=False).mean()\n",
    "\n",
    "# Pivot CPI data to have a column for each MEASURE\n",
    "cpi_data_pivot = cpi_data.pivot(index='Year', columns='MEASURE', values='CPI_Value').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881f49f-a106-45a4-a42a-643110d0999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_data_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188dbca6-edb2-4440-ac33-0cab63ef7613",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_data_pivot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e30669-b18e-4df9-9630-88ea2eab80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Consumer_Price_Index_Monthly = pd.read_csv('Consumer Price Index/Consumer_Price_Index_Monthly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426fe9a-eb1c-458d-a4be-7d295ec5a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Monthly CPI Data\n",
    "monthly_cpi_data = Consumer_Price_Index_Monthly[['MEASURE', 'TIME_PERIOD', 'OBS_VALUE']].copy()\n",
    "\n",
    "# Extract Year and Month from TIME_PERIOD\n",
    "monthly_cpi_data['Year'] = pd.to_datetime(monthly_cpi_data['TIME_PERIOD']).dt.year\n",
    "monthly_cpi_data['Month'] = pd.to_datetime(monthly_cpi_data['TIME_PERIOD']).dt.month\n",
    "\n",
    "# Rename columns for clarity\n",
    "monthly_cpi_data.rename(columns={'OBS_VALUE': 'CPI_Value'}, inplace=True)\n",
    "\n",
    "# Ensure CPI_Value is numeric\n",
    "monthly_cpi_data['CPI_Value'] = pd.to_numeric(monthly_cpi_data['CPI_Value'], errors='coerce')\n",
    "\n",
    "# Step 2: Handle duplicate entries (aggregate by Year, Month, and MEASURE)\n",
    "# Aggregate only numeric data\n",
    "monthly_cpi_data = monthly_cpi_data.groupby(['Year', 'Month', 'MEASURE'], as_index=False)['CPI_Value'].mean()\n",
    "\n",
    "# Step 3: Pivot the Monthly CPI Data\n",
    "monthly_cpi_pivot = monthly_cpi_data.pivot(\n",
    "    index=['Year', 'Month'], \n",
    "    columns='MEASURE', \n",
    "    values='CPI_Value'\n",
    ").reset_index()\n",
    "\n",
    "# Check the processed data\n",
    "print(monthly_cpi_pivot.info())\n",
    "print(monthly_cpi_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cb0dc-b13f-43a0-a58a-7f99c1fd9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_data2 = merged_data.merge(monthly_cpi_pivot, on=[\"Year\", \"Month\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffec704-9eed-4518-8bdd-ede77f46ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42790402-c7a9-4fbb-923e-eaba34048f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Consumer_Price_Index_Quarterly = pd.read_csv('Consumer Price Index/Consumer_Price_Index_Quarterly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553bc3b-34b2-4164-b05f-29d279a26775",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consumer_Price_Index_Quarterly.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47921cb-acf4-459d-9c39-637a1360c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Quarterly CPI Data\n",
    "quarterly_cpi_data = Consumer_Price_Index_Quarterly[['MEASURE', 'TIME_PERIOD', 'OBS_VALUE']].copy()\n",
    "\n",
    "# Extract Year and Quarter from TIME_PERIOD\n",
    "quarterly_cpi_data['Year'] = quarterly_cpi_data['TIME_PERIOD'].str[:4].astype(int)  # Extract Year\n",
    "quarterly_cpi_data['Quarter'] = quarterly_cpi_data['TIME_PERIOD'].str[-2:]  # Extract Quarter (e.g., 'Q1', 'Q2')\n",
    "\n",
    "# Rename columns for clarity\n",
    "quarterly_cpi_data.rename(columns={'OBS_VALUE': 'CPI_Value'}, inplace=True)\n",
    "\n",
    "# Ensure CPI_Value is numeric\n",
    "quarterly_cpi_data['CPI_Value'] = pd.to_numeric(quarterly_cpi_data['CPI_Value'], errors='coerce')\n",
    "\n",
    "# Step 2: Handle duplicate entries (aggregate by Year, Quarter, and MEASURE)\n",
    "quarterly_cpi_data = quarterly_cpi_data.groupby(['Year', 'Quarter', 'MEASURE'], as_index=False)['CPI_Value'].mean()\n",
    "\n",
    "# Step 3: Pivot the Quarterly CPI Data\n",
    "quarterly_cpi_pivot = quarterly_cpi_data.pivot(\n",
    "    index=['Year', 'Quarter'], \n",
    "    columns='MEASURE', \n",
    "    values='CPI_Value'\n",
    ").reset_index()\n",
    "\n",
    "# Check the processed data\n",
    "print(quarterly_cpi_pivot.info())\n",
    "print(quarterly_cpi_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc67e3a-3735-4cab-9e6d-07716a724653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Currency_Strength = pd.read_csv('Currency Strength/AED-USD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a85dc9-ab59-4e30-96f8-2a78afb482ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Currency Strength Data\n",
    "currency_data = Currency_Strength[['Date', 'Close', 'Return']].copy()\n",
    "\n",
    "# Step 2: Convert Date column to datetime\n",
    "currency_data['Date'] = pd.to_datetime(currency_data['Date'], errors='coerce')\n",
    "\n",
    "# Step 3: Extract Year and Month\n",
    "currency_data['Year'] = currency_data['Date'].dt.year\n",
    "currency_data['Month'] = currency_data['Date'].dt.month\n",
    "\n",
    "# Step 4: Handle duplicates (aggregate by Year and Month)\n",
    "currency_data_aggregated = currency_data.groupby(['Year', 'Month'], as_index=False).agg({\n",
    "    'Close': 'mean',    # Average close value for each month\n",
    "    'Return': 'mean'    # Average return for each month\n",
    "})\n",
    "\n",
    "# Step 5: Rename Columns for Clarity\n",
    "currency_data_aggregated.rename(columns={\n",
    "    'Close': 'Average_Close',\n",
    "    'Return': 'Average_Return'\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 6: Validate the Processed Data\n",
    "print(currency_data_aggregated.info())\n",
    "print(currency_data_aggregated.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad9228-8cb6-427c-8fc8-f72a98729a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_data_aggregated.rename(columns={\n",
    "    \"Average_Close\": \"AEDUSD_Average_Close\",\n",
    "    \"Average_Return\": \"AEDUSD_Average_Return\"\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754a962-f2da-4810-ad94-4e2d3b5a62d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_data3 = merged_data2.merge(currency_data_aggregated, on=[\"Year\", \"Month\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af6247-096b-4a38-aabe-293d521fd2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb683f36-18ea-4794-810e-3ce2e3b0cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "GDP_Quarterly_Constant_Prices = pd.read_csv('Gross Domestic Product/GDP_Quarterly_Constant_Prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b030e-9070-461b-b284-2b7c9030e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP_Quarterly_Constant_Prices.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986afa1-e30d-4554-8744-4d38ebe64488",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP_Quarterly_Constant_Prices['MEASURE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6f6a4-4093-45d3-8169-b2504f1e8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP_Quarterly_Constant_Prices['TIME_PERIOD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65a8aa-ac6b-484d-80e7-a261a2c21c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the GDP Quarterly Data\n",
    "gdp_quarterly_data = GDP_Quarterly_Constant_Prices[['TIME_PERIOD', 'MEASURE', 'OBS_VALUE']].copy()\n",
    "\n",
    "# Step 2: Rename 'TIME_PERIOD' to 'Year' directly since it already represents the year\n",
    "gdp_quarterly_data.rename(columns={'TIME_PERIOD': 'Year', 'OBS_VALUE': 'GDP_Value'}, inplace=True)\n",
    "\n",
    "# Step 3: Extract Quarter from the 'QUARTER' column\n",
    "gdp_quarterly_data['Quarter'] = GDP_Quarterly_Constant_Prices['QUARTER']\n",
    "\n",
    "# Step 4: Handle Duplicate Entries\n",
    "# Ensure no duplicates by aggregating using .mean(), but adjust if necessary\n",
    "gdp_quarterly_data = gdp_quarterly_data.groupby(['Year', 'Quarter', 'MEASURE'], as_index=False).mean()\n",
    "\n",
    "# Step 5: Pivot the Data\n",
    "gdp_quarterly_pivot = gdp_quarterly_data.pivot(\n",
    "    index=['Year', 'Quarter'], \n",
    "    columns='MEASURE', \n",
    "    values='GDP_Value'\n",
    ").reset_index()\n",
    "\n",
    "# Step 6: Fill Missing Values (Optional)\n",
    "# Fill missing values with 0 or another placeholder, as appropriate\n",
    "gdp_quarterly_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Step 7: Validate the Processed Data\n",
    "print(gdp_quarterly_pivot.info())\n",
    "print(gdp_quarterly_pivot.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25515fc-0d95-4bb4-a715-45c5143f4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ACC – Accommodation & Hospitality (Hotels, Restaurants, Tourism)\n",
    "ACT – Activities (General services, business activities, or arts and entertainment)\n",
    "AGR – Agriculture, Forestry & Fishing\n",
    "ART – Arts, Culture & Recreation\n",
    "CON – Construction\n",
    "EDU – Education\n",
    "ELE – Electricity, Gas & Water Supply (Energy sector)\n",
    "FIN – Financial Services (Banking, Insurance, Investments)\n",
    "HUM – Human Health & Social Work Activities\n",
    "INF – Information & Communication (Telecom, IT services, Media)\n",
    "MAN – Manufacturing\n",
    "MIN – Mining & Quarrying\n",
    "NFC – Non-Financial Corporations (Could be general businesses excluding financial institutions)\n",
    "PRO – Professional, Scientific & Technical Activities\n",
    "PUB – Public Administration & Defense (Government services)\n",
    "REA – Real Estate Activities\n",
    "TOT_GDP – Total GDP (Overall economic output)\n",
    "TOT_NO – Total Number (Could be employment figures or total enterprises)\n",
    "TRA – Transportation & Storage\n",
    "WHO – Wholesale & Retail Trade\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034736f2-60cf-4ead-962e-ebaff1e2a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Quarter to Year by averaging\n",
    "yearly_gdp_data = gdp_quarterly_pivot.drop(columns=[\"Quarter\"]).groupby(\"Year\", as_index=False).mean()\n",
    "\n",
    "yearly_gdp_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a863a-59e3-4266-a7cf-18195bf063c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_data4 = merged_data3.merge(yearly_gdp_data, on=\"Year\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95df2d-d693-4b92-97eb-966afc49badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "GDP_Quarterly_Current_Prices = pd.read_csv('Gross Domestic Product/GDP_Quarterly_Current_Prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5edbf8-7b40-4511-afde-b4f620844b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the GDP Quarterly Current Prices Data\n",
    "gdp_quarterly_current_data = GDP_Quarterly_Current_Prices[['TIME_PERIOD', 'MEASURE', 'OBS_VALUE']].copy()\n",
    "\n",
    "# Step 2: Rename 'TIME_PERIOD' to 'Year' directly since it already represents the year\n",
    "gdp_quarterly_current_data.rename(columns={'TIME_PERIOD': 'Year', 'OBS_VALUE': 'GDP_Value'}, inplace=True)\n",
    "\n",
    "# Step 3: Extract Quarter from the 'QUARTER' column\n",
    "gdp_quarterly_current_data['Quarter'] = GDP_Quarterly_Current_Prices['QUARTER']\n",
    "\n",
    "# Step 4: Handle Duplicate Entries\n",
    "# Ensure no duplicates by aggregating using .mean(), but adjust if necessary\n",
    "gdp_quarterly_current_data = gdp_quarterly_current_data.groupby(['Year', 'Quarter', 'MEASURE'], as_index=False).mean()\n",
    "\n",
    "# Step 5: Pivot the Data\n",
    "gdp_quarterly_current_pivot = gdp_quarterly_current_data.pivot(\n",
    "    index=['Year', 'Quarter'], \n",
    "    columns='MEASURE', \n",
    "    values='GDP_Value'\n",
    ").reset_index()\n",
    "\n",
    "# Step 6: Fill Missing Values (Optional)\n",
    "# Fill missing values with 0 or another placeholder, as appropriate\n",
    "gdp_quarterly_current_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Step 7: Validate the Processed Data\n",
    "print(gdp_quarterly_current_pivot.info())\n",
    "print(gdp_quarterly_current_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581a6c6-8250-4f10-94e3-e5edc80c5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Quarter to Year by averaging\n",
    "gdp_yearly_current = gdp_quarterly_current_pivot.drop(columns=[\"Quarter\"]).groupby(\"Year\", as_index=False).mean()\n",
    "\n",
    "gdp_yearly_current.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fabcc6-b88f-4301-be05-2e75a76cb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_yearly_current.rename(columns={col: col + \"_current\" for col in gdp_yearly_current.columns if col != \"Year\"}, inplace=True)\n",
    "gdp_yearly_current.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40da259-79f5-4f62-914f-f98fc3bdbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_data5 = merged_data4.merge(gdp_yearly_current, on=\"Year\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a22d13-bff9-45c9-a756-cc2c66c17499",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d42c4-3ca7-4393-a1f0-f9f9d706f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Population_Gender = pd.read_csv('Population/Population_Estimates_and_Growth_by_Gender.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d8b2a-4556-4747-b06d-700a33a848ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Population_Gender.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe4a0b5-fe31-43d2-b975-4af7f426c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "Population_Gender['GENDER'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9b5c3-04af-495f-9740-3797ceab63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Population by Gender Data\n",
    "population_gender_data = Population_Gender[['TIME_PERIOD', 'GENDER', 'OBS_VALUE']].copy()\n",
    "\n",
    "# Step 2: Rename 'TIME_PERIOD' to 'Year' and 'OBS_VALUE' to 'Population'\n",
    "population_gender_data.rename(columns={'TIME_PERIOD': 'Year', 'OBS_VALUE': 'Population'}, inplace=True)\n",
    "\n",
    "# Step 3: Pivot the Data\n",
    "# Create separate columns for Male and Female populations based on the 'GENDER' column\n",
    "population_gender_pivot = population_gender_data.pivot(\n",
    "    index='Year',\n",
    "    columns='GENDER',\n",
    "    values='Population'\n",
    ").reset_index()\n",
    "\n",
    "# Step 4: Fill Missing Values (Optional)\n",
    "# Fill missing values with 0 or another placeholder, as appropriate\n",
    "population_gender_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Step 5: Validate the Processed Data\n",
    "print(population_gender_pivot.info())\n",
    "print(population_gender_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac098547-f9cd-4424-bb14-e30e5140e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Rename\n",
    "population_gender_pivot.rename(columns={\n",
    "    \"F\": \"Population_F\",\n",
    "    \"M\": \"Population_M\",\n",
    "    \"_T\": \"Population_Total\"\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd2f2d-cfa9-41b9-b866-644dcd2b0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_data6 = merged_data5.merge(population_gender_pivot, on=\"Year\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725fdaa-54bb-4d3e-a56c-cbdf72a007a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b44bd-d61c-4001-a355-de2802cb78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Population_Indicators = pd.read_csv('Population/Population_Indicators.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9099863b-be34-4be9-8a37-7e68179a6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Population Indicators Data\n",
    "population_indicators_data = Population_Indicators[['TIME_PERIOD', 'MEASURE', 'OBS_VALUE']].copy()\n",
    "\n",
    "# Step 2: Rename 'TIME_PERIOD' to 'Year' and 'OBS_VALUE' to 'Value'\n",
    "population_indicators_data.rename(columns={'TIME_PERIOD': 'Year', 'OBS_VALUE': 'Value'}, inplace=True)\n",
    "\n",
    "# Step 3: Handle Duplicate Entries\n",
    "# Aggregate using .mean() (or another aggregation method like sum) if there are duplicate Year + MEASURE combinations\n",
    "population_indicators_data = population_indicators_data.groupby(['Year', 'MEASURE'], as_index=False).mean()\n",
    "\n",
    "# Step 4: Pivot the Data\n",
    "# Pivot the data based on the 'MEASURE' column to separate indicators into individual columns\n",
    "population_indicators_pivot = population_indicators_data.pivot(\n",
    "    index='Year',\n",
    "    columns='MEASURE',\n",
    "    values='Value'\n",
    ").reset_index()\n",
    "\n",
    "# Step 5: Fill Missing Values (Optional)\n",
    "# Fill missing values with 0 or another placeholder, as appropriate\n",
    "population_indicators_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Step 6: Validate the Processed Data\n",
    "print(population_indicators_pivot.info())\n",
    "print(population_indicators_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044866e-23d2-4b24-9368-c17cf974dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Guests_by_Hotel_Type_by_Region = pd.read_csv('Tourism/Guests_by_Hotel_Type_by_Region.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4243a-d66c-494e-8bf3-53baea158187",
   "metadata": {},
   "outputs": [],
   "source": [
    "Guests_by_Hotel_Type_by_Region.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ade44-5bd0-4623-b499-bc4ac95e8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Guests_by_Hotel_Type_by_Region['TIME_PERIOD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df080c0c-559a-44da-bb28-15c1f640da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Guests_by_Hotel_Type_by_Region['H_TYPE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f0ba8-5fa0-4649-9ef8-bcb237e588e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Guests_by_Hotel_Type_by_Region['GUEST_REGION'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb73edd-cfc2-474c-a6df-6e797b3de073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter out relevant columns\n",
    "guests_data = Guests_by_Hotel_Type_by_Region[['TIME_PERIOD', 'GUEST_REGION', 'OBS_VALUE']]\n",
    "\n",
    "# Step 2: Pivot the table to have regions as columns\n",
    "guests_pivot = guests_data.pivot_table(index='TIME_PERIOD', columns='GUEST_REGION', values='OBS_VALUE', aggfunc='sum')\n",
    "\n",
    "# Step 3: Reset the index to make 'TIME_PERIOD' a column\n",
    "guests_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Step 4: Rename the columns to make them more descriptive\n",
    "guests_pivot.rename(columns={'TIME_PERIOD': 'Year'}, inplace=True)\n",
    "\n",
    "# Now, the dataframe will have a structure with Year and each region's guest values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de8daa-d971-41a5-857f-fa8fba02bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "guests_pivot.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ea799-7896-471f-b8af-69bdced54fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Rename\n",
    "# Step 5: Rename the columns with 'GuestNumber_' prefix\n",
    "guests_pivot.columns = ['Year'] + ['GuestNumber_' + col for col in guests_pivot.columns[1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c9d02-8812-4b6d-9b48-c0d107d669f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "guests_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b60c82-fb26-4e3b-bdb2-efe71d9a82b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "guests_pivot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79299dc4-78eb-41c2-bb15-079df6e10ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_data7 = merged_data6.merge(guests_pivot, on=\"Year\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f8db26-0d94-4786-955f-028b3e0aea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data7.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925a650-5cab-4519-9fcb-d77205249708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Hotel_Establishments_and_Rooms_by_Rating_Type = pd.read_csv('Tourism/Hotel_Establishments_and_Rooms_by_Rating_Type.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518d4b2-a6fa-4ac6-b70b-c57bf66d4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hotel_Establishments_and_Rooms_by_Rating_Type.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46704c7-f5da-4dc0-976f-0dd4538eb4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hotel_Establishments_and_Rooms_by_Rating_Type['MEASURE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8278e-2b2a-4e78-968e-747ebf8a342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hotel_Establishments_and_Rooms_by_Rating_Type['REF_AREA'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f1add-f4d5-4c1c-b170-6051e3b08cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hotel_Establishments_and_Rooms_by_Rating_Type['H_TYPE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50184e07-a5c9-4992-a704-1eb4ccb2c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hotel_Establishments_and_Rooms_by_Rating_Type['H_INDICATOR'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd384d8-dcee-4269-a6d4-aaced474efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Data\n",
    "hotel_data = Hotel_Establishments_and_Rooms_by_Rating_Type[['TIME_PERIOD', 'H_TYPE', 'H_INDICATOR', 'OBS_VALUE']].copy()\n",
    "\n",
    "# Step 2: Rename the columns for better understanding\n",
    "hotel_data.rename(columns={'TIME_PERIOD': 'Year', 'OBS_VALUE': 'Count'}, inplace=True)\n",
    "\n",
    "# Step 3: Handle Duplicates by Aggregating the Data\n",
    "# Aggregate using .sum() for duplicate entries of Year, H_TYPE, and H_INDICATOR\n",
    "hotel_data = hotel_data.groupby(['Year', 'H_TYPE', 'H_INDICATOR'], as_index=False).sum()\n",
    "\n",
    "# Step 4: Pivot the Data\n",
    "# We pivot based on 'Year' as the index, and 'H_TYPE' and 'H_INDICATOR' as columns\n",
    "hotel_data_pivot = hotel_data.pivot_table(\n",
    "    index='Year',\n",
    "    columns=['H_TYPE', 'H_INDICATOR'],\n",
    "    values='Count',\n",
    "    aggfunc='sum'\n",
    ").reset_index()\n",
    "\n",
    "# Step 5: Handle Missing Values (Optional)\n",
    "# Fill missing values with 0 or another placeholder\n",
    "hotel_data_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Step 6: Validate the Processed Data\n",
    "print(hotel_data_pivot.info())\n",
    "print(hotel_data_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d70f3-82d3-4929-8a55-3fa6eb093def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the columns by renaming the 'Year' column\n",
    "hotel_data_pivot.columns = ['Year' if col == ('Year', '') else col[0] + '_' + col[1] for col in hotel_data_pivot.columns]\n",
    "\n",
    "# Now, verify the columns again\n",
    "print(hotel_data_pivot.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233c221-c0db-4098-ade2-3adb5baaa035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_data8 = merged_data7.merge(hotel_data_pivot, on=\"Year\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667bda4-d517-4e8e-9db0-4d850dfcf316",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data8.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea50b36-5eb1-4499-8ad8-1c396aa07781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Hotel_Establishments_Main_Indicators = pd.read_csv('Tourism/Hotel_Establishments_Main_Indicators.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b08b3a-2bcf-44fd-89f3-27e5493891eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Data\n",
    "hotel_main_data = Hotel_Establishments_Main_Indicators[['TIME_PERIOD', 'H_TYPE', 'H_INDICATOR', 'OBS_VALUE']].copy()\n",
    "\n",
    "# Step 2: Rename the columns for better understanding\n",
    "hotel_main_data.rename(columns={'TIME_PERIOD': 'Year', 'OBS_VALUE': 'Revenue'}, inplace=True)\n",
    "\n",
    "# Step 3: Handle Duplicates by Aggregating the Data\n",
    "# Aggregate using .sum() for duplicate entries of Year, H_TYPE, and H_INDICATOR\n",
    "hotel_main_data = hotel_main_data.groupby(['Year', 'H_TYPE', 'H_INDICATOR'], as_index=False).sum()\n",
    "\n",
    "# Step 4: Pivot the Data\n",
    "# We pivot based on 'Year' as the index, and 'H_TYPE' and 'H_INDICATOR' as columns\n",
    "hotel_main_data_pivot = hotel_main_data.pivot_table(\n",
    "    index='Year',\n",
    "    columns=['H_TYPE', 'H_INDICATOR'],\n",
    "    values='Revenue',\n",
    "    aggfunc='sum'\n",
    ").reset_index()\n",
    "\n",
    "# Step 5: Handle Missing Values (Optional)\n",
    "# Fill missing values with 0 or another placeholder\n",
    "hotel_main_data_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Step 6: Validate the Processed Data\n",
    "print(hotel_main_data_pivot.info())\n",
    "print(hotel_main_data_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9ca282-752a-4c02-b572-a693ce1c4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RR - Total Revenue (likely the most important indicator)\n",
    "TOR - Total Occupancy Rate (percentage of rooms occupied)\n",
    "GUN - Gross Utilization Number (likely measures the occupancy or usage rate of hotel resources)\n",
    "LS - Length of Stay (average duration of stay for guests)\n",
    "FB - Food & Beverage Revenue (likely indicates revenue from dining services)\n",
    "AR - Average Room Rate (the average price of a room per night)\n",
    "OR - Occupancy Rate (similar to TOR, but could be more specific in context)\n",
    "TR - Total Rooms (total number of rooms in the establishment)\n",
    "TAR - Total Available Rooms (could indicate the number of rooms available for booking)\n",
    "ARR - Average Room Revenue (average revenue per room)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd3c31-f15c-4ff9-a158-e0a8e2418f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the columns by renaming the 'Year' column\n",
    "hotel_main_data_pivot.columns = ['Year' if col == ('Year', '') else col[0] + '_' + col[1] for col in hotel_main_data_pivot.columns]\n",
    "\n",
    "# Now, verify the columns again\n",
    "print(hotel_main_data_pivot.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea6ed0-f232-4cf0-84cf-f79254351141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_data9 = merged_data8.merge(hotel_main_data_pivot, on=\"Year\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea4f4f0-6d3c-4700-b45e-c65bea999118",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data9.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51bbe41-2e10-4484-9344-d8158238dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "World_Development_Indicator = pd.read_csv('World Development Indicators/World_Development_Indicator.csv',\n",
    "                                          skiprows=4,            # Skip the first 4 rows\n",
    "                                          delimiter=\",\",         # Specify the delimiter\n",
    "                                          quotechar='\"',         # Handle quoted fields\n",
    "                                          engine=\"python\"        # Use Python engine for flexibility\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf45414-016e-4a2b-866f-691c7801fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Melt the Data\n",
    "world_dev_data = World_Development_Indicator.drop(columns=['Country Code','Country Name', 'Unnamed: 68'])  # Drop any irrelevant columns\n",
    "world_dev_data_melted = world_dev_data.melt(id_vars=['Indicator Name'],\n",
    "                                            var_name='Year', \n",
    "                                            value_name='Value')\n",
    "\n",
    "# Step 2: Pivot the Data\n",
    "# We will pivot the data so that each row corresponds to a specific indicator and year for a country\n",
    "world_dev_data_pivot = world_dev_data_melted.pivot_table(\n",
    "    index=[ 'Year'],\n",
    "    columns='Indicator Name',\n",
    "    values='Value',\n",
    "    aggfunc='first'  # Take the first value in case of duplicates\n",
    ").reset_index()\n",
    "\n",
    "# Step 3: Clean the Data (Optional)\n",
    "# You can fill missing values with NaN or 0, depending on your preference\n",
    "world_dev_data_pivot.fillna(0, inplace=True)\n",
    "\n",
    "# Step 4: Validate the Processed Data\n",
    "print(world_dev_data_pivot.info())\n",
    "print(world_dev_data_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b47d2d-560c-433f-8ff4-5d790ea2ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Replace 0 values with NaN to treat them as missing\n",
    "world_dev_data_pivot_no_zeros = world_dev_data_pivot.replace(0, pd.NA)\n",
    "\n",
    "# Step 2: Calculate the percentage of missing values for each indicator (column), considering 0 as missing\n",
    "missing_percentage = world_dev_data_pivot_no_zeros.drop(columns=['Year']).isnull().mean() * 100\n",
    "\n",
    "# Step 3: Debugging: Check the percentage of missing values for the first few indicators\n",
    "print(missing_percentage.head())\n",
    "\n",
    "# Step 4: Filter out indicators with missing data above a certain threshold (e.g., 5%)\n",
    "threshold = 4\n",
    "selected_columns = missing_percentage[missing_percentage < threshold].index\n",
    "\n",
    "# Step 5: Debugging: Check the selected columns after applying the threshold\n",
    "print(f\"Selected columns after thresholding: {selected_columns.tolist()}\")\n",
    "\n",
    "# Step 6: Filter the dataset to keep 'Year' and the selected columns\n",
    "world_dev_data_selected = world_dev_data_pivot[['Year'] + selected_columns.tolist()]\n",
    "\n",
    "# Step 7: Validate the filtered data\n",
    "print(world_dev_data_selected.info())\n",
    "print(world_dev_data_selected.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfcd50-1f74-47ff-89aa-50888cb1686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_dev_data_selected.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5340ea-deb5-4024-9c22-8370e0bd137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of desired columns to keep\n",
    "columns_to_keep = [\n",
    "    'Year',\n",
    "    'Birth rate, crude (per 1,000 people)',\n",
    "    'Death rate, crude (per 1,000 people)',\n",
    "    'Fertility rate, total (births per woman)',\n",
    "    'Life expectancy at birth, total (years)',\n",
    "    'Net migration',\n",
    "    'Population, total',\n",
    "    'Rural population',\n",
    "    'Urban population'\n",
    "]\n",
    "\n",
    "# Filter the dataframe to keep only the desired columns\n",
    "world_dev_data_filtered = world_dev_data_selected[columns_to_keep]\n",
    "\n",
    "# Validate the result\n",
    "world_dev_data_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30379dd6-0231-4152-8cb0-b682de783ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_dev_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34b437-8d99-4a48-a14a-2ce14b89cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert columns to numeric using .loc[]\n",
    "columns_to_convert = [\n",
    "    'Year',\n",
    "    'Birth rate, crude (per 1,000 people)', \n",
    "    'Death rate, crude (per 1,000 people)', \n",
    "    'Fertility rate, total (births per woman)', \n",
    "    'Life expectancy at birth, total (years)', \n",
    "    'Net migration', \n",
    "    'Population, total', \n",
    "    'Rural population', \n",
    "    'Urban population'\n",
    "]\n",
    "\n",
    "# Convert columns to numeric, coercing errors to NaN\n",
    "for col in columns_to_convert:\n",
    "    world_dev_data_filtered.loc[:, col] = pd.to_numeric(world_dev_data_filtered[col], errors='coerce')\n",
    "\n",
    "# Check the result\n",
    "print(world_dev_data_filtered.info())\n",
    "print(world_dev_data_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472120b-0e05-4480-adfb-ab252cd3759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, convert it to an integer, replacing NaN with a placeholder (e.g., 0)\n",
    "world_dev_data_filtered['Year'] = world_dev_data_filtered['Year'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8bd4d3-0cf0-42fe-99a2-eb5ac9818765",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_dev_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27bb47e-d516-42fa-adea-eea51bc9a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Rename columns for clarity\n",
    "world_dev_data_filtered.rename(columns={\n",
    "    'Birth rate, crude (per 1,000 people)': 'BirthRate_crude',\n",
    "    'Death rate, crude (per 1,000 people)': 'DeathRate_crude',\n",
    "    'Fertility rate, total (births per woman)': 'FertilityRate_total',\n",
    "    'Life expectancy at birth, total (years)': 'LifeExpectancy_birth',\n",
    "    'Net migration': 'NetMigration',\n",
    "    'Population, total': 'Population_total',\n",
    "    'Rural population': 'RuralPopulation',\n",
    "    'Urban population': 'UrbanPopulation'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4f9c6-ad05-40bb-aae0-2f8d34ce5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "final_merged_data = merged_data9.merge(world_dev_data_filtered, on=\"Year\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecf120-c0ca-43cc-8b65-35332bbf4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84cdb55-6f81-4c84-b596-c5dc4e9c2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c62f0c-ec23-4664-bce6-1240a87b2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_merged_data.to_csv('final_merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35baa16a-a6dc-411e-8128-4b77bcbb1360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6c7c6-a33b-4e82-bfbd-38cd847101e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63246f-451c-447e-b9c1-d29ae265b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - Annual Amount on Final Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09e5fe-4730-4bad-8771-43cd7ce04e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# 1. Prepare the data\n",
    "# Drop rows where 'Annual Amount' is missing\n",
    "final_merged_data = final_merged_data.dropna(subset=['Annual Amount'])\n",
    "\n",
    "# Define features and target\n",
    "X = final_merged_data.drop(columns=['Annual Amount', 'Ejari Contract Number', 'Registration Date', 'Start Date', 'End Date', 'Transaction Date'])\n",
    "y = final_merged_data['Annual Amount']\n",
    "\n",
    "# 2. Train-Test Split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efd103-9dcf-44c4-8a69-0345f4b67df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost Model\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Handle NaN in categorical features\n",
    "cat_columns = X_train.select_dtypes(include=['object']).columns\n",
    "for col in cat_columns:\n",
    "    X_train[col] = X_train[col].fillna('Unknown')\n",
    "    X_test[col] = X_test[col].fillna('Unknown')\n",
    "\n",
    "# Categorical features: Get the indices of the categorical columns in X_train\n",
    "cat_features = [i for i, col in enumerate(X_train.columns) if X_train[col].dtype == 'object']\n",
    "\n",
    "# Step 1: Define the CatBoost model\n",
    "model = CatBoostRegressor(iterations=200,  # Number of boosting iterations\n",
    "                          learning_rate=0.1,  # Learning rate\n",
    "                          depth=6,  # Tree depth\n",
    "                          cat_features=cat_features,  # List of categorical feature indices\n",
    "                          random_seed=42,  # Random seed for reproducibility\n",
    "                          verbose=200)  # Print progress every 200 iterations\n",
    "\n",
    "# Step 2: Train the model\n",
    "model.fit(X_train, y_train, cat_features=cat_features)\n",
    "\n",
    "# Step 3: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)  # Calculate R² Score\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"MSE: {mse:.4f}\")   # Mean Squared Error\n",
    "print(f\"RMSE: {rmse:.4f}\")  # Root Mean Squared Error\n",
    "print(f\"R² Score: {r2:.4f}\")  # R-squared score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4757d58-de24-4989-95dc-f3c26c32535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importance from the trained model\n",
    "feature_importance = model.get_feature_importance()\n",
    "\n",
    "# Create a DataFrame to map feature names with importance scores\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort the importance values in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select the top 30 (or change to 40 if needed)\n",
    "top_n = 30  # Change to 40 if needed\n",
    "top_features_df = feature_importance_df.head(top_n)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_features_df['Feature'], top_features_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance of Annual Amount for CatBoost (Final Merged Data)')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important features on top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07beedab-3b4b-4376-a1b1-4c30d4d50b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Model\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Step 1: Convert categorical columns to category dtype\n",
    "categorical_columns = ['Version', 'Area', 'Is Free Hold?', 'Property Type', \n",
    "                       'Property Sub Type', 'Usage', 'Nearest Metro', \n",
    "                       'Nearest Mall', 'Nearest Landmark', 'Master Project', 'Project',\n",
    "                      'Transaction Number', 'Transaction Type', 'Transaction sub type',\n",
    "                       'Registration type', 'Room(s)',\n",
    "                       'BirthRate_crude', 'DeathRate_crude',\n",
    "       'FertilityRate_total', 'LifeExpectancy_birth', 'NetMigration',\n",
    "       'Population_total', 'RuralPopulation', 'UrbanPopulation'\n",
    "                      ]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "# Step 2: Define the LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(objective='regression', \n",
    "                              num_iterations=200,  # Number of boosting iterations\n",
    "                              learning_rate=0.1,  # Learning rate\n",
    "                              max_depth=6,  # Tree depth\n",
    "                              verbose=-1,\n",
    "                              random_state=42)  # Random seed for reproducibility\n",
    "\n",
    "# Step 3: Train the model with categorical features\n",
    "lgb_model.fit(X_train, y_train, categorical_feature=categorical_columns)\n",
    "\n",
    "# Step 4: Make predictions on the test set\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "mse_lgb = mean_squared_error(y_test, y_pred_lgb)\n",
    "rmse_lgb = mse_lgb ** 0.5\n",
    "r2_lgb = r2_score(y_test, y_pred_lgb)  # Calculate R² Score\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"MSE: {mse_lgb:.4f}\")   # Mean Squared Error\n",
    "print(f\"RMSE: {rmse_lgb:.4f}\")  # Root Mean Squared Error\n",
    "print(f\"R² Score: {r2_lgb:.4f}\")  # R-squared score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad432d9-14e8-4c32-ae0a-b30e15edef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Plot feature importance for LightGBM\n",
    "lgb_feature_importance = lgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to map feature names with importance scores\n",
    "lgb_feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': lgb_feature_importance\n",
    "})\n",
    "\n",
    "# Sort the importance values in ascending order\n",
    "lgb_feature_importance_df = lgb_feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select the top 30 (or change to 40 if needed)\n",
    "top_n = 30  # Change to 40 if needed\n",
    "lgb_top_features_df = lgb_feature_importance_df.head(top_n)\n",
    "\n",
    "# Plot LightGBM feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(lgb_top_features_df['Feature'], lgb_top_features_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance of Annual Amount for LightGBM (Final Merged Data)')\n",
    "plt.gca().invert_yaxis()  # Most important features on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0361db98-4d12-4e87-b3c4-2525e17049a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2e216-0728-4ecd-814a-63b746fa6f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289af59-e50e-4581-b564-bba0ef1f0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - Amount on Sales Data - ['Transaction Type'] == 'Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60f765-3eed-42b1-be7e-1c43f1304038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to include only 'Sales' transaction type\n",
    "merged_sales_data = final_merged_data[final_merged_data['Transaction Type'] == 'Sales']\n",
    "\n",
    "# Check the result\n",
    "print(merged_sales_data.info())\n",
    "print(merged_sales_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c3ad1-37ef-4b66-9629-0ac74c2d4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_sales_data.to_csv('merged_sales_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b68b07-02b8-4221-8619-532fa6fc36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# 1. Prepare the data\n",
    "# Drop rows where 'Annual Amount' is missing\n",
    "merged_sales_data = merged_sales_data.dropna(subset=['Amount'])\n",
    "\n",
    "# Define features and target\n",
    "X = merged_sales_data.drop(columns=['Amount', 'Ejari Contract Number', 'Registration Date', 'Start Date', 'End Date', 'Transaction Date'])\n",
    "y = merged_sales_data['Amount']\n",
    "\n",
    "# 2. Train-Test Split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1158b15a-7613-4b7c-a9e5-126244e04f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost Model\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Handle NaN in categorical features\n",
    "cat_columns = X_train.select_dtypes(include=['object']).columns\n",
    "for col in cat_columns:\n",
    "    X_train[col] = X_train[col].fillna('Unknown')\n",
    "    X_test[col] = X_test[col].fillna('Unknown')\n",
    "\n",
    "# Categorical features: Get the indices of the categorical columns in X_train\n",
    "cat_features = [i for i, col in enumerate(X_train.columns) if X_train[col].dtype == 'object']\n",
    "\n",
    "# Step 1: Define the CatBoost model\n",
    "model = CatBoostRegressor(iterations=200,  # Number of boosting iterations\n",
    "                          learning_rate=0.1,  # Learning rate\n",
    "                          depth=6,  # Tree depth\n",
    "                          cat_features=cat_features,  # List of categorical feature indices\n",
    "                          random_seed=42,  # Random seed for reproducibility\n",
    "                          verbose=200)  # Print progress every 200 iterations\n",
    "\n",
    "# Step 2: Train the model\n",
    "model.fit(X_train, y_train, cat_features=cat_features)\n",
    "\n",
    "# Step 3: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)  # Calculate R² Score\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"MSE: {mse:.4f}\")   # Mean Squared Error\n",
    "print(f\"RMSE: {rmse:.4f}\")  # Root Mean Squared Error\n",
    "print(f\"R² Score: {r2:.4f}\")  # R-squared score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923128cb-ad4e-4244-b810-32f899b5fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importance from the trained model\n",
    "feature_importance = model.get_feature_importance()\n",
    "\n",
    "# Create a DataFrame to map feature names with importance scores\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort the importance values in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select the top 30 (or change to 40 if needed)\n",
    "top_n = 30  # Change to 40 if needed\n",
    "top_features_df = feature_importance_df.head(top_n)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_features_df['Feature'], top_features_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance of Amount for CatBoost (Final Merged Data (Sales))')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important features on top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194eea6-cba7-4d5a-bfd8-1852abbb103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Model\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Step 1: Convert categorical columns to category dtype\n",
    "categorical_columns = ['Version', 'Area', 'Is Free Hold?', 'Property Type', \n",
    "                       'Property Sub Type', 'Usage', 'Nearest Metro', \n",
    "                       'Nearest Mall', 'Nearest Landmark', 'Master Project', 'Project',\n",
    "                      'Transaction Number', 'Transaction Type', 'Transaction sub type',\n",
    "                       'Registration type', 'Room(s)',\n",
    "                       'BirthRate_crude', 'DeathRate_crude',\n",
    "       'FertilityRate_total', 'LifeExpectancy_birth', 'NetMigration',\n",
    "       'Population_total', 'RuralPopulation', 'UrbanPopulation'\n",
    "                      ]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "# Step 2: Define the LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(objective='regression', \n",
    "                              num_iterations=200,  # Number of boosting iterations\n",
    "                              learning_rate=0.1,  # Learning rate\n",
    "                              max_depth=6,  # Tree depth\n",
    "                              verbose=-1,\n",
    "                              random_state=42)  # Random seed for reproducibility\n",
    "\n",
    "# Step 3: Train the model with categorical features\n",
    "lgb_model.fit(X_train, y_train, categorical_feature=categorical_columns)\n",
    "\n",
    "# Step 4: Make predictions on the test set\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "mse_lgb = mean_squared_error(y_test, y_pred_lgb)\n",
    "rmse_lgb = mse_lgb ** 0.5\n",
    "r2_lgb = r2_score(y_test, y_pred_lgb)  # Calculate R² Score\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"MSE: {mse_lgb:.4f}\")   # Mean Squared Error\n",
    "print(f\"RMSE: {rmse_lgb:.4f}\")  # Root Mean Squared Error\n",
    "print(f\"R² Score: {r2_lgb:.4f}\")  # R-squared score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44222a-7922-48d2-a77a-113639000a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Plot feature importance for LightGBM\n",
    "lgb_feature_importance = lgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to map feature names with importance scores\n",
    "lgb_feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': lgb_feature_importance\n",
    "})\n",
    "\n",
    "# Sort the importance values in ascending order\n",
    "lgb_feature_importance_df = lgb_feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select the top 30 (or change to 40 if needed)\n",
    "top_n = 30  # Change to 40 if needed\n",
    "lgb_top_features_df = lgb_feature_importance_df.head(top_n)\n",
    "\n",
    "# Plot LightGBM feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(lgb_top_features_df['Feature'], lgb_top_features_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance of Amount for LightGBM (Final Merged Data (Sales))')\n",
    "plt.gca().invert_yaxis()  # Most important features on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916cc58-ff60-48ee-9491-c6af3d0060c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the correlations between these macroeconomic factors and property sale or rental prices. \n",
    "# Highlight the most significant factors driving market behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd8d96-7916-4d83-bb96-d99be2c47cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all column names\n",
    "print(merged_sales_data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002b121-19d6-48fb-a7de-7e25c4f6a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of macroeconomic factors (adjust based on dataset)\n",
    "macro_factors = [\n",
    "    '_Z_TOR', '_Z_TR', 'BirthRate_crude', 'DeathRate_crude', 'FertilityRate_total',\n",
    "    'LifeExpectancy_birth', 'NetMigration', 'Population_total', \n",
    "    'RuralPopulation', 'UrbanPopulation'\n",
    "]\n",
    "\n",
    "# Select only relevant columns\n",
    "macro_data = merged_sales_data[macro_factors + ['Annual Amount', 'Amount']]\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = macro_data.corr()\n",
    "\n",
    "# Extract correlation with 'Annual Amount' and 'Amount'\n",
    "correlation_annual = correlation_matrix['Annual Amount'].drop(['Annual Amount', 'Amount'])\n",
    "correlation_amount = correlation_matrix['Amount'].drop(['Annual Amount', 'Amount'])\n",
    "\n",
    "# Display correlation values\n",
    "print(\"Correlation with 'Annual Amount':\\n\", correlation_annual)\n",
    "print(\"\\nCorrelation with 'Amount':\\n\", correlation_amount)\n",
    "\n",
    "# Visualizing correlations with heatmaps\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Heatmap for 'Annual Amount'\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(correlation_annual.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation with Annual Amount\")\n",
    "\n",
    "# Heatmap for 'Amount'\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(correlation_amount.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation with Amount\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d8f41-6474-4bca-9b54-73ab6a573483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of macroeconomic factors (adjust based on dataset)\n",
    "macro_factors = [\n",
    "    'BirthRate_crude', 'DeathRate_crude', 'FertilityRate_total',\n",
    "    'LifeExpectancy_birth', 'NetMigration',  \n",
    "    'RuralPopulation', 'UrbanPopulation',\n",
    "    'Contract Amount', 'Property Size (sq.m)', 'Transaction Size (sq.m)',\n",
    "    'Prev_Month_Avg_Price', 'Prev_Week_Avg_Price',\n",
    "    'CPI_ANNCHG', 'CPI_ANNCHG21', 'CPI_INDEX14', 'CPI_INDEX21', 'CPI_MTHCHG',\n",
    "    'AEDUSD_Average_Close', 'AEDUSD_Average_Return', 'TOT_GDP', 'TOT_NO', 'TOT_GDP_current', 'TOT_NO_current',\n",
    "    'Population_F', 'Population_M', 'Population_total'\n",
    "]\n",
    "\n",
    "# Select only relevant columns\n",
    "macro_data = merged_sales_data[macro_factors + ['Annual Amount', 'Amount']]\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = macro_data.corr()\n",
    "\n",
    "# Extract correlation with 'Annual Amount' and 'Amount'\n",
    "correlation_annual = correlation_matrix['Annual Amount'].drop(['Annual Amount', 'Amount'])\n",
    "correlation_amount = correlation_matrix['Amount'].drop(['Annual Amount', 'Amount'])\n",
    "\n",
    "# Display correlation values\n",
    "print(\"Correlation with 'Annual Amount':\\n\", correlation_annual)\n",
    "print(\"\\nCorrelation with 'Amount':\\n\", correlation_amount)\n",
    "\n",
    "# Visualizing correlations with heatmaps\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Heatmap for 'Annual Amount'\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(correlation_annual.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation with Annual Amount\")\n",
    "\n",
    "# Heatmap for 'Amount'\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(correlation_amount.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation with Amount\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3dfa50-198a-49b1-9a3d-02cebd9568cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
